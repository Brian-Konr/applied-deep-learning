{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADL HW1 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12922050/miniconda3/envs/adl_hw1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset swag (/home/guest/r12922050/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n",
      "100%|██████████| 3/3 [00:00<00:00, 480.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 73546\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "swag = load_dataset('swag', 'regular')\n",
    "\n",
    "# take a look at swag dataset to see what it looks like\n",
    "swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>relevant</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593f14f960d971e294af884f0194b3a7</td>\n",
       "      <td>舍本和誰的數據能推算出連星的恆星的質量？</td>\n",
       "      <td>[2018, 6952, 8264, 836]</td>\n",
       "      <td>836</td>\n",
       "      <td>{'text': '斯特魯維', 'start': 108}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acd5d763ec4c250f9a11eac1412d6814</td>\n",
       "      <td>在關西鎮以什麼方言為主？</td>\n",
       "      <td>[1716, 8318, 4070, 7571]</td>\n",
       "      <td>8318</td>\n",
       "      <td>{'text': '四縣腔客家話', 'start': 306}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5670000714a658c5e52658e22e6985f7</td>\n",
       "      <td>「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?</td>\n",
       "      <td>[6043, 5950, 2548, 5806]</td>\n",
       "      <td>5806</td>\n",
       "      <td>{'text': '王翦', 'start': 46}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f71493751ccfce42aae7a99ed3f20dbb</td>\n",
       "      <td>《方法論》的作者是誰?</td>\n",
       "      <td>[5443, 6843, 8584, 4350]</td>\n",
       "      <td>6843</td>\n",
       "      <td>{'text': '阿基米德', 'start': 63}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56d547357d11a8938197072d82aa126c</td>\n",
       "      <td>環繞速度又為哪種速度的別稱?</td>\n",
       "      <td>[1859, 7590, 3116, 8989]</td>\n",
       "      <td>7590</td>\n",
       "      <td>{'text': '第一宇宙速度', 'start': 69}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21709</th>\n",
       "      <td>0ba8689b6665bb8d700807e586244018</td>\n",
       "      <td>又稱爲虎斑的為什麼?</td>\n",
       "      <td>[5662, 68, 7394, 92]</td>\n",
       "      <td>7394</td>\n",
       "      <td>{'text': '土衛二', 'start': 213}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21710</th>\n",
       "      <td>9405202eb2a8391136a5905987ebab26</td>\n",
       "      <td>愛爾蘭王國的王位在哪一年正式獲得神權認可?</td>\n",
       "      <td>[641, 6490, 3870, 2146]</td>\n",
       "      <td>3870</td>\n",
       "      <td>{'text': '1555年', 'start': 47}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21711</th>\n",
       "      <td>8dda88faa2c6b074a48b66cb74662727</td>\n",
       "      <td>《自題小像》是誰的作品?</td>\n",
       "      <td>[1991, 3309, 4956, 6311]</td>\n",
       "      <td>1991</td>\n",
       "      <td>{'text': '魯迅', 'start': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21712</th>\n",
       "      <td>92484786f021da4ed4dcf952401a7402</td>\n",
       "      <td>日蝕和月蝕在何時代就可以被算出來了？</td>\n",
       "      <td>[2745, 5391, 928, 4386]</td>\n",
       "      <td>928</td>\n",
       "      <td>{'text': '古希臘', 'start': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21713</th>\n",
       "      <td>7faf3194e307cde1832a21c2d999fa17</td>\n",
       "      <td>牛津大學在2004年與哪家公司合作進行大規模的數位化系統更新項目？</td>\n",
       "      <td>[1502, 8215, 1156, 8824]</td>\n",
       "      <td>1156</td>\n",
       "      <td>{'text': '谷歌', 'start': 235}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21714 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0      593f14f960d971e294af884f0194b3a7   \n",
       "1      acd5d763ec4c250f9a11eac1412d6814   \n",
       "2      5670000714a658c5e52658e22e6985f7   \n",
       "3      f71493751ccfce42aae7a99ed3f20dbb   \n",
       "4      56d547357d11a8938197072d82aa126c   \n",
       "...                                 ...   \n",
       "21709  0ba8689b6665bb8d700807e586244018   \n",
       "21710  9405202eb2a8391136a5905987ebab26   \n",
       "21711  8dda88faa2c6b074a48b66cb74662727   \n",
       "21712  92484786f021da4ed4dcf952401a7402   \n",
       "21713  7faf3194e307cde1832a21c2d999fa17   \n",
       "\n",
       "                                        question                paragraphs  \\\n",
       "0                           舍本和誰的數據能推算出連星的恆星的質量？   [2018, 6952, 8264, 836]   \n",
       "1                                   在關西鎮以什麼方言為主？  [1716, 8318, 4070, 7571]   \n",
       "2      「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?  [6043, 5950, 2548, 5806]   \n",
       "3                                    《方法論》的作者是誰?  [5443, 6843, 8584, 4350]   \n",
       "4                                 環繞速度又為哪種速度的別稱?  [1859, 7590, 3116, 8989]   \n",
       "...                                          ...                       ...   \n",
       "21709                                 又稱爲虎斑的為什麼?      [5662, 68, 7394, 92]   \n",
       "21710                      愛爾蘭王國的王位在哪一年正式獲得神權認可?   [641, 6490, 3870, 2146]   \n",
       "21711                               《自題小像》是誰的作品?  [1991, 3309, 4956, 6311]   \n",
       "21712                         日蝕和月蝕在何時代就可以被算出來了？   [2745, 5391, 928, 4386]   \n",
       "21713          牛津大學在2004年與哪家公司合作進行大規模的數位化系統更新項目？  [1502, 8215, 1156, 8824]   \n",
       "\n",
       "       relevant                            answer  \n",
       "0           836    {'text': '斯特魯維', 'start': 108}  \n",
       "1          8318  {'text': '四縣腔客家話', 'start': 306}  \n",
       "2          5806       {'text': '王翦', 'start': 46}  \n",
       "3          6843     {'text': '阿基米德', 'start': 63}  \n",
       "4          7590   {'text': '第一宇宙速度', 'start': 69}  \n",
       "...         ...                               ...  \n",
       "21709      7394     {'text': '土衛二', 'start': 213}  \n",
       "21710      3870    {'text': '1555年', 'start': 47}  \n",
       "21711      1991        {'text': '魯迅', 'start': 0}  \n",
       "21712       928       {'text': '古希臘', 'start': 0}  \n",
       "21713      1156      {'text': '谷歌', 'start': 235}  \n",
       "\n",
       "[21714 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfer my dataset to data format like swag\n",
    "\n",
    "import pandas as pd\n",
    "train_data = pd.read_json('./data/train.json')\n",
    "\n",
    "# take a look at train_data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9010</th>\n",
       "      <td>因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9013 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...\n",
       "1     這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...\n",
       "2     處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...\n",
       "3     福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...\n",
       "4     盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...\n",
       "...                                                 ...\n",
       "9008  在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...\n",
       "9009  雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...\n",
       "9010  因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...\n",
       "9011  因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...\n",
       "9012  針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...\n",
       "\n",
       "[9013 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loda context.json\n",
    "context = pd.read_json('./data/context.json')\n",
    "\n",
    "# take a look at context_data\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            593f14f960d971e294af884f0194b3a7\n",
       "question                  舍本和誰的數據能推算出連星的恆星的質量？\n",
       "paragraphs             [2018, 6952, 8264, 836]\n",
       "relevant                                   836\n",
       "answer          {'text': '斯特魯維', 'start': 108}\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first row of train_data\n",
    "train_data.iloc[0]\n",
    "# paragraphs are 4 context ids that represent 4 possible choices, and the id can refer to context.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first row of swag\n",
    "swag['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>relevant</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593f14f960d971e294af884f0194b3a7</td>\n",
       "      <td>舍本和誰的數據能推算出連星的恆星的質量？</td>\n",
       "      <td>[2018, 6952, 8264, 836]</td>\n",
       "      <td>836</td>\n",
       "      <td>{'text': '斯特魯維', 'start': 108}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acd5d763ec4c250f9a11eac1412d6814</td>\n",
       "      <td>在關西鎮以什麼方言為主？</td>\n",
       "      <td>[1716, 8318, 4070, 7571]</td>\n",
       "      <td>8318</td>\n",
       "      <td>{'text': '四縣腔客家話', 'start': 306}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5670000714a658c5e52658e22e6985f7</td>\n",
       "      <td>「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?</td>\n",
       "      <td>[6043, 5950, 2548, 5806]</td>\n",
       "      <td>5806</td>\n",
       "      <td>{'text': '王翦', 'start': 46}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f71493751ccfce42aae7a99ed3f20dbb</td>\n",
       "      <td>《方法論》的作者是誰?</td>\n",
       "      <td>[5443, 6843, 8584, 4350]</td>\n",
       "      <td>6843</td>\n",
       "      <td>{'text': '阿基米德', 'start': 63}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56d547357d11a8938197072d82aa126c</td>\n",
       "      <td>環繞速度又為哪種速度的別稱?</td>\n",
       "      <td>[1859, 7590, 3116, 8989]</td>\n",
       "      <td>7590</td>\n",
       "      <td>{'text': '第一宇宙速度', 'start': 69}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  593f14f960d971e294af884f0194b3a7   \n",
       "1  acd5d763ec4c250f9a11eac1412d6814   \n",
       "2  5670000714a658c5e52658e22e6985f7   \n",
       "3  f71493751ccfce42aae7a99ed3f20dbb   \n",
       "4  56d547357d11a8938197072d82aa126c   \n",
       "\n",
       "                                    question                paragraphs  \\\n",
       "0                       舍本和誰的數據能推算出連星的恆星的質量？   [2018, 6952, 8264, 836]   \n",
       "1                               在關西鎮以什麼方言為主？  [1716, 8318, 4070, 7571]   \n",
       "2  「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?  [6043, 5950, 2548, 5806]   \n",
       "3                                《方法論》的作者是誰?  [5443, 6843, 8584, 4350]   \n",
       "4                             環繞速度又為哪種速度的別稱?  [1859, 7590, 3116, 8989]   \n",
       "\n",
       "   relevant                            answer  \n",
       "0       836    {'text': '斯特魯維', 'start': 108}  \n",
       "1      8318  {'text': '四縣腔客家話', 'start': 306}  \n",
       "2      5806       {'text': '王翦', 'start': 46}  \n",
       "3      6843     {'text': '阿基米德', 'start': 63}  \n",
       "4      7590   {'text': '第一宇宙速度', 'start': 69}  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 5 rows of train_data\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "- for train and valid, use `load_dataset`\n",
    "- for context, it's a normal json file, use `pd.read_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ed466149ca31f82a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 3276.80it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 253.96it/s]\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 716.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'paragraphs', 'relevant', 'answer'],\n",
       "        num_rows: 21714\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'paragraphs', 'relevant', 'answer'],\n",
       "        num_rows: 3009\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "\n",
    "my_datasets = load_dataset('json', data_files=data_files)\n",
    "my_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context.json\n",
    "- after reading it, rename it to paragraph_ref_map for index reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9010</th>\n",
       "      <td>因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9013 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...\n",
       "1     這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...\n",
       "2     處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...\n",
       "3     福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...\n",
       "4     盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...\n",
       "...                                                 ...\n",
       "9008  在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...\n",
       "9009  雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...\n",
       "9010  因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...\n",
       "9011  因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...\n",
       "9012  針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...\n",
       "\n",
       "[9013 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read context.json\n",
    "context_df = pd.read_json('./data/context.json')\n",
    "context_df\n",
    "\n",
    "paragraph_ref_map = {}\n",
    "# enumerate context_df\n",
    "for i, row in context_df.iterrows():\n",
    "    # add context_df to context\n",
    "    paragraph_ref_map[i] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_ref_map = {}\n",
    "# enumerate context_df\n",
    "for i, row in context_df.iterrows():\n",
    "    # add context_df to context\n",
    "    paragraph_ref_map[i] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固的女的一面或雙面蒙上拉緊的膜。鼓可以用手或鼓杵敲擊出聲。鼓在非洲的傳統音樂以及在現代音樂中是一種比較重要的樂器，有的樂隊完全由以鼓為主的打擊樂器組成。鼓除了作為樂器外，在古代許多文明中還用鼓來傳播信息。不同類型的鼓，如定音鼓等，均被調校至特定的音調中。更常見的是將不同類型的鼓或打擊樂器互相組合，以構成常於流行音樂出現的爵士鼓。鼓的演奏一般是用手敲擊鼓面，或是用一二隻鼓棒或鼓錘敲擊。由於鼓的觸覺特性及其易於使用，在音樂治療中常用到鼓，特別是手鼓。在許多傳統文化中，鼓有其象徵的意義，也常用在宗教儀式中。像在蒲隆地的卡央達鼓是王權的象徵，卡央達鼓也出現在1962至1966年間的蒲隆地國旗中。在流行音樂或爵士樂中，鼓常常是指由一組鼓及銅鈸組成的爵士鼓，演奏者稱為鼓手。鼓幾乎都有一個圓形的開口，鼓面拉緊後可以固定在上面。但鼓身的形狀就有很多的變化，西洋樂器的鼓，鼓身多半都是圓柱體，但定音鼓的鼓身則是碗形，有些鼓的鼓身則是截角圓錐或是二個接合的截角圓錐。中國、日本、韓國的鼓常常是中間略寬、上下略窄的圓柱體。最早的鼓是出現於西元前六千年的兩河文明。\n",
      "盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡述伊壁鳩魯的哲學思想和德謨克利特的原子論，表示物質不滅、凡人不必懼怕死亡的生活觀。盧克萊修是古羅馬文學史上著名的智者，維吉爾曾稱羨慕他知曉事物的起因，是個幸福的人。卡圖魯斯生於義大利北部維羅那一個富有的家庭，經常出入羅馬上流社會，是黃金時代成就最高的抒情詩人。他是堅定不移的共和派，曾公然反對過愷撒，曾創作過許多辛辣的諷刺短詩。卡圖魯斯的詩作現存116首，他善用警句式的語言表達濃鬱熱烈、複雜微妙的感情。他的抒情詩對後世歐洲許多偉大詩人都產生過影響。賀拉斯出生於拍賣商家庭，是和卡圖魯斯齊名的抒情詩人。他幼年受過良好的教育，通曉拉丁語和希臘語，能誦荷馬史詩原文，併到雅典學過哲學。他的代表作品包括《長短句集》17首和《閒談集》18首。前者表明作者反對內戰，幻想黃金時代到來的思想；後者則諷刺羅馬社會的惡習。但賀拉斯最著名的作品是後期的《歌集》和《詩藝》。賀拉斯的抒情詩改造了希臘抒情詩的格律，構思巧妙，語言優美，優雅莊重，以有意、愛情、詩藝為題，融哲理和感情於一路，不少人競相模仿。《詩藝》則是古羅馬時期文藝理論上的最高成就，被古典主義文學視為經典。\n",
      "雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理論研究了八年之久。這期間他面臨的一個主要問題是缺乏有效的數學工具，直到1913年他在德國數學家馬塞爾·格羅斯曼的幫助下發表了一篇突破性的論文：《廣義相對論和重力理論綱要》，題目標註了物理部分作者為愛因斯坦，數學部分作者為格羅斯曼。這篇論文中原來單一的牛頓純量重力場被一個具有十個分量的四階對稱黎曼張量重力場替代，從此物理學中的時空不再是平直的，而成為了在全局上具有曲率但在局部平直的黎曼流形。1914年，愛因斯坦發表了《廣義相對論正式基礎》，其中他得到了廣義相對論中描述粒子運動的方程式：測地線方程式，並籍此推導了重力場中的光線偏折和重力紅移的結果。1915年11月，愛因斯坦發表了他最終推導出重力場方程式的四篇論文，其中《用廣義相對論解釋水星近日點運動》證明了廣義相對論能夠解釋自1859年以來困擾天文學家的水星的反常進動現象，而《重力場方程式》則正式給出了描述重力場和物質交互作用的愛因斯坦重力場方程式。\n"
     ]
    }
   ],
   "source": [
    "# check some ids\n",
    "print(paragraph_ref_map[0])\n",
    "print(paragraph_ref_map[4])\n",
    "print(paragraph_ref_map[9009])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: swag/regular\n",
      "Found cached dataset swag (/home/guest/r12922050/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n",
      "100%|██████████| 3/3 [00:00<00:00, 583.03it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"swag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2118.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 1]\n",
      "first sentences: [['Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession walk down the street holding small horn brass instruments.'], ['A drum line passes by walking down the street playing their instruments.', 'A drum line passes by walking down the street playing their instruments.', 'A drum line passes by walking down the street playing their instruments.', 'A drum line passes by walking down the street playing their instruments.'], ['A group of members in green uniforms walks waving flags.', 'A group of members in green uniforms walks waving flags.', 'A group of members in green uniforms walks waving flags.', 'A group of members in green uniforms walks waving flags.']]\n",
      "second sentences: [['A drum line passes by walking down the street playing their instruments.', 'A drum line has heard approaching them.', \"A drum line arrives and they're outside dancing and asleep.\", 'A drum line turns the lead singer watches the performance.'], ['Members of the procession are playing ping pong and celebrating one left each in quick.', 'Members of the procession wait slowly towards the cadets.', 'Members of the procession continues to play as well along the crowd along with the band being interviewed.', 'Members of the procession continue to play marching, interspersed.'], ['Members of the procession pay the other coaches to cheer as people this chatter dips in lawn sheets.', 'Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession is seen in the background.', 'Members of the procession are talking a couple of people playing a game of tug of war.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1992.54ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2]\n",
      "first sentences: [['Students lower their eyes nervously.', 'Students lower their eyes nervously.', 'Students lower their eyes nervously.', 'Students lower their eyes nervously.'], ['He rides the motorcycle down the hall and into the elevator.', 'He rides the motorcycle down the hall and into the elevator.', 'He rides the motorcycle down the hall and into the elevator.', 'He rides the motorcycle down the hall and into the elevator.'], ['The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.', 'The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.', 'The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.', 'The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.']]\n",
      "second sentences: [['She pats her shoulder, then saunters toward someone.', 'She turns with two students.', 'She walks slowly towards someone.', 'She wheels around as her dog thunders out.'], ['He looks at a mirror in the mirror as he watches someone walk through a door.', \"He stops, listening to a cup of coffee with the seated woman, who's standing.\", 'He exits the building and rides the motorcycle into a casino where he performs several tricks as people watch.', \"He pulls the bag out of his pocket and hands it to someone's grandma.\"], ['He shoots a look at her.', 'He makes his way past it and peers out a window.', 'He rides the motorcycle down the hall and into the elevator.', 'He sits on the ground beside her pants, clinging by a flannel wool.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2114.06ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1]\n",
      "first sentences: [['A person shows the bottom of a large dust mop.', 'A person shows the bottom of a large dust mop.', 'A person shows the bottom of a large dust mop.', 'A person shows the bottom of a large dust mop.'], ['The camera move and we see a man next to the lady.', 'The camera move and we see a man next to the lady.', 'The camera move and we see a man next to the lady.', 'The camera move and we see a man next to the lady.'], ['The lady stops and adjusts the camera.', 'The lady stops and adjusts the camera.', 'The lady stops and adjusts the camera.', 'The lady stops and adjusts the camera.']]\n",
      "second sentences: [['Then, the person places the scrub on the floor then cleans the soap evenly.', 'Then, the person cleans a gym with the large dust mop.', 'Then, the person stops and spray a car with some spray paint.', 'Then, the person places the bucket and a bucket.'], ['the lady stands on a bench in the bar.', 'the lady put a piece down on the table.', 'the lady fixes her hair in the camera.', 'the lady completes a picture and shows a standing room.'], ['the camera move and we see a man going down the street.', 'the camera move and we see a lady kick a scene.', 'the camera move and we see a man next to the lady.', 'the camera move and we see a closeup of her hair before sitting down again.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# trim samples\n",
    "for split in raw_datasets.keys():\n",
    "    raw_datasets[split] = raw_datasets[split].select(range(3))\n",
    "\n",
    "if raw_datasets[\"train\"] is not None:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "else:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "\n",
    "ending_names = [f\"ending{i}\" for i in range(4)]\n",
    "context_name = \"sent1\"\n",
    "question_header_name = \"sent2\"\n",
    "label_column_name = \"label\" if \"label\" in column_names else \"labels\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[context_name]]\n",
    "    # size of first_sentences\n",
    "    question_headers = examples[question_header_name]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "    labels = examples[label_column_name]\n",
    "    print(labels)\n",
    "    print(f\"first sentences: {first_sentences}\")\n",
    "    print(f\"second sentences: {second_sentences}\")\n",
    "    # Flatten out\n",
    "    first_sentences = list(chain(*first_sentences))\n",
    "    second_sentences = list(chain(*second_sentences))\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '593f14f960d971e294af884f0194b3a7',\n",
       " 'question': '舍本和誰的數據能推算出連星的恆星的質量？',\n",
       " 'paragraphs': [2018, 6952, 8264, 836],\n",
       " 'relevant': 836,\n",
       " 'answer': {'start': 108, 'text': '斯特魯維'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ed466149ca31f82a\n",
      "Found cached dataset json (/home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 2/2 [00:00<00:00, 410.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "\n",
    "my_datasets = load_dataset('json', data_files=data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim samples\n",
    "for split in my_datasets.keys():\n",
    "    my_datasets[split] = my_datasets[split].select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 10.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 624/624 [00:00<00:00, 1.19MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 37.3MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 269k/269k [00:00<00:00, 497kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator(gradient_accumulation_steps=2)\n",
    "from itertools import chain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "if my_datasets[\"train\"] is not None:\n",
    "    column_names = my_datasets[\"train\"].column_names\n",
    "else:\n",
    "    column_names = my_datasets[\"validation\"].column_names\n",
    "\n",
    "context_name = \"question\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[context_name]]\n",
    "    second_sentences = []\n",
    "    labels = []\n",
    "    for i, paragraphs in enumerate(examples['paragraphs']):\n",
    "        correct_paragraph = examples['relevant'][i]\n",
    "        possible_ans = []\n",
    "        for ans_idx, paragraph in enumerate(paragraphs):\n",
    "            if paragraph == correct_paragraph:\n",
    "                labels.append(ans_idx)\n",
    "            possible_ans.append(paragraph_ref_map[paragraph])\n",
    "        second_sentences.append(possible_ans)\n",
    "\n",
    "    # Flatten out\n",
    "    first_sentences = list(chain(*first_sentences))\n",
    "    second_sentences = list(chain(*second_sentences))\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        max_length=args.max_seq_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # Un-flatten\n",
    "    tokenized_inputs = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with accelerator.main_process_first():\n",
    "    processed_datasets = my_datasets.map(\n",
    "        preprocess_function, batched=True, remove_columns=my_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 13428 of the training set: 0.\n",
      "{'id': 'a6407580654244d6efdb31db964726ce', 'question': '印度全境炎熱，大部分屬於何種氣候?', 'paragraphs': [1812, 4272, 4849, 3443], 'relevant': 1812, 'answer': {'start': 744, 'text': '熱帶季風'}}\n"
     ]
    }
   ],
   "source": [
    "index = 13428\n",
    "print(f\"Sample {index} of the training set: {processed_datasets['train'][index]['labels']}.\")\n",
    "print(my_datasets[\"train\"][13428])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12922050/miniconda3/envs/adl-hw1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.43kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 90.8kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 626kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.48MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/guest/r12922050/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "\n",
    "# trim samples\n",
    "for split in squad.keys():\n",
    "    squad[split] = squad[split].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733f18ad058e614000b6644',\n",
       " 'title': 'Montana',\n",
       " 'context': 'The name Montana comes from the Spanish word Montaña, meaning \"mountain\", or more broadly, \"mountainous country\". Montaña del Norte was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was successfully changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained that Montana had \"no meaning\". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given that most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as Shoshone were suggested, but it was eventually decided that the Committee on Territories could name it whatever they wanted, so the original name of Montana was adopted.',\n",
       " 'question': 'What did the Spanish call this region?',\n",
       " 'answers': {'text': ['Montaña del Norte'], 'answer_start': [114]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: answer_start 的 value 代表的是 index，因此像 155 就代表是第 155 個字\n",
    "\n",
    "- answers: the starting location of the answer token and the answer text.\n",
    "- context: background information from which the model needs to extract the answer.\n",
    "- question: the question a model should answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = squad[\"train\"].column_names\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "args = {'doc_stride': 128}\n",
    "\n",
    "max_seq_length = min(384, tokenizer.model_max_length)\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    print(f\"questions: {examples[question_column_name]}\")\n",
    "    print(f\"contexts: {examples[context_column_name]}\")\n",
    "    # check size of contexts\n",
    "    print(f\"size of contexts: {len(examples[context_column_name])}\")\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if True else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        print(f\"answers: {answers}\")\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`truncation=\"only_second\"` means only the second sequence will be truncated to fit the maximum length specified by `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 114.87ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions: ['What did the Spanish call this region?', 'What film did Beyonce appear in with Mike Myers?', \"What is the annual budget of Notre Dame's LaFortune Center?\", \"What publication's partial reprinting gave the book wide public exposure?\", 'What city saw the largest growth?']\n",
      "contexts: ['The name Montana comes from the Spanish word Montaña, meaning \"mountain\", or more broadly, \"mountainous country\". Montaña del Norte was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was successfully changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained that Montana had \"no meaning\". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given that most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as Shoshone were suggested, but it was eventually decided that the Committee on Territories could name it whatever they wanted, so the original name of Montana was adopted.', 'In July 2002, Beyoncé continued her acting career playing Foxxy Cleopatra alongside Mike Myers in the comedy film, Austin Powers in Goldmember, which spent its first weekend atop the US box office and grossed $73 million. Beyoncé released \"Work It Out\" as the lead single from its soundtrack album which entered the top ten in the UK, Norway, and Belgium. In 2003, Beyoncé starred opposite Cuba Gooding, Jr., in the musical comedy The Fighting Temptations as Lilly, a single mother whom Gooding\\'s character falls in love with. The film received mixed reviews from critics but grossed $30 million in the U.S. Beyoncé released \"Fighting Temptation\" as the lead single from the film\\'s soundtrack album, with Missy Elliott, MC Lyte, and Free which was also used to promote the film. Another of Beyoncé\\'s contributions to the soundtrack, \"Summertime\", fared better on the US charts.', 'A Science Hall was built in 1883 under the direction of Fr. Zahm, but in 1950 it was converted to a student union building and named LaFortune Center, after Joseph LaFortune, an oil executive from Tulsa, Oklahoma. Commonly known as \"LaFortune\" or \"LaFun,\" it is a 4-story building of 83,000 square feet that provides the Notre Dame community with a meeting place for social, recreational, cultural, and educational activities. LaFortune employs 35 part-time student staff and 29 full-time non-student staff and has an annual budget of $1.2 million. Many businesses, services, and divisions of The Office of Student Affairs are found within. The building also houses restaurants from national restaurant chains.', 'Ultimately, Lee spent over two and a half years writing To Kill a Mockingbird. The book was published on July 11, 1960. After rejecting the \"Watchman\" title, it was initially re-titled Atticus, but Lee renamed it \"To Kill a Mockingbird\" to reflect that the story went beyond just a character portrait. The editorial team at Lippincott warned Lee that she would probably sell only several thousand copies. In 1964, Lee recalled her hopes for the book when she said, \"I never expected any sort of success with \\'Mockingbird.\\' ... I was hoping for a quick and merciful death at the hands of the reviewers but, at the same time, I sort of hoped someone would like it enough to give me encouragement. Public encouragement. I hoped for a little, as I said, but I got rather a whole lot, and in some ways this was just about as frightening as the quick, merciful death I\\'d expected.\" Instead of a \"quick and merciful death\", Reader\\'s Digest Condensed Books chose the book for reprinting in part, which gave it a wide readership immediately. Since the original publication, the book has never been out of print.', \"The United States Census Bureau estimates that the population of Montana was 1,032,949 on July 1, 2015, a 4.40% increase since the 2010 United States Census. The 2010 census put Montana's population at 989,415 which is an increase of 43,534 people, or 4.40 percent, since 2010. During the first decade of the new century, growth was mainly concentrated in Montana's seven largest counties, with the highest percentage growth in Gallatin County, which saw a 32 percent increase in its population from 2000-2010. The city seeing the largest percentage growth was Kalispell with 40.1 percent, and the city with the largest increase in actual residents was Billings with an increase in population of 14,323 from 2000-2010.\"]\n",
      "size of contexts: 5\n",
      "answers: {'text': ['Montaña del Norte'], 'answer_start': [114]}\n",
      "answers: {'text': ['Austin Powers in Goldmember'], 'answer_start': [115]}\n",
      "answers: {'text': ['$1.2 million'], 'answer_start': [535]}\n",
      "answers: {'text': [\"Reader's Digest Condensed Books\"], 'answer_start': [917]}\n",
      "answers: {'text': ['Kalispell'], 'answer_start': [561]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 150.80ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions: ['Catholic people identified with Notre Dame, what religious group did people feel Yale represented?', 'Who did Beyoncé tour with for the Verizon Lades First Tour?', 'What were the words Chopin wrote to a friend when he was alone and homesick?', 'What address did Frédéric live at during his stay in Paris?', 'How many certifications did RIAA give Beyoncé?']\n",
      "contexts: [\"The success of its football team made Notre Dame a household name. The success of Note Dame reflected rising status of Irish Americans and Catholics in the 1920s. Catholics rallied up around the team and listen to the games on the radio, especially when it knocked off the schools that symbolized the Protestant establishment in America — Harvard, Yale, Princeton, and Army. Yet this role as high-profile flagship institution of Catholicism made it an easy target of anti-Catholicism. The most remarkable episode of violence was the clash between Notre Dame students and the Ku Klux Klan in 1924. Nativism and anti-Catholicism, especially when directed towards immigrants, were cornerstones of the KKK's rhetoric, and Notre Dame was seen as a symbol of the threat posed by the Catholic Church. The Klan decided to have a week-long Klavern in South Bend. Clashes with the student body started on March 17, when students, aware of the anti-Catholic animosity, blocked the Klansmen from descending from their trains in the South Bend station and ripped the KKK clothes and regalia. On May 19 thousands of students massed downtown protesting the Klavern, and only the arrival of college president Fr. Matthew Walsh prevented any further clashes. The next day, football coach Knute Rockne spoke at a campus rally and implored the students to obey the college president and refrain from further violence. A few days later the Klavern broke up, but the hostility shown by the students was an omen and a contribution to the downfall of the KKK in Indiana.\", 'In November 2003, she embarked on the Dangerously in Love Tour in Europe and later toured alongside Missy Elliott and Alicia Keys for the Verizon Ladies First Tour in North America. On February 1, 2004, Beyoncé performed the American national anthem at Super Bowl XXXVIII, at the Reliant Stadium in Houston, Texas. After the release of Dangerously in Love, Beyoncé had planned to produce a follow-up album using several of the left-over tracks. However, this was put on hold so she could concentrate on recording Destiny Fulfilled, the final studio album by Destiny\\'s Child. Released on November 15, 2004, in the US and peaking at number two on the Billboard 200, Destiny Fulfilled included the singles \"Lose My Breath\" and \"Soldier\", which reached the top five on the Billboard Hot 100 chart. Destiny\\'s Child embarked on a worldwide concert tour, Destiny Fulfilled... and Lovin\\' It and during the last stop of their European tour, in Barcelona on June 11, 2005, Rowland announced that Destiny\\'s Child would disband following the North American leg of the tour. The group released their first compilation album Number 1\\'s on October 25, 2005, in the US and accepted a star on the Hollywood Walk of Fame in March 2006.', 'Chopin\\'s successes as a composer and performer opened the door to western Europe for him, and on 2 November 1830, he set out, in the words of Zdzisław Jachimecki, \"into the wide world, with no very clearly defined aim, forever.\" With Woyciechowski, he headed for Austria, intending to go on to Italy. Later that month, in Warsaw, the November 1830 Uprising broke out, and Woyciechowski returned to Poland to enlist. Chopin, now alone in Vienna, was nostalgic for his homeland, and wrote to a friend, \"I curse the moment of my departure.\" When in September 1831 he learned, while travelling from Vienna to Paris, that the uprising had been crushed, he expressed his anguish in the pages of his private journal: \"Oh God! ... You are there, and yet you do not take vengeance!\" Jachimecki ascribes to these events the composer\\'s maturing \"into an inspired national bard who intuited the past, present and future of his native Poland.\"', \"The two became friends, and for many years lived in close proximity in Paris, Chopin at 38 Rue de la Chaussée-d'Antin, and Liszt at the Hôtel de France on the Rue Lafitte, a few blocks away. They performed together on seven occasions between 1833 and 1841. The first, on 2 April 1833, was at a benefit concert organized by Hector Berlioz for his bankrupt Shakespearean actress wife Harriet Smithson, during which they played George Onslow's Sonata in F minor for piano duet. Later joint appearances included a benefit concert for the Benevolent Association of Polish Ladies in Paris. Their last appearance together in public was for a charity concert conducted for the Beethoven Memorial in Bonn, held at the Salle Pleyel and the Paris Conservatory on 25 and 26 April 1841.\", 'Beyoncé has received numerous awards. As a solo artist she has sold over 15 million albums in the US, and over 118 million records worldwide (a further 60 million additionally with Destiny\\'s Child), making her one of the best-selling music artists of all time. The Recording Industry Association of America (RIAA) listed Beyoncé as the top certified artist of the 2000s, with a total of 64 certifications. Her songs \"Crazy in Love\", \"Single Ladies (Put a Ring on It)\", \"Halo\", and \"Irreplaceable\" are some of the best-selling singles of all time worldwide. In 2009, The Observer named her the Artist of the Decade and Billboard named her the Top Female Artist and Top Radio Songs Artist of the Decade. In 2010, Billboard named her in their \"Top 50 R&B/Hip-Hop Artists of the Past 25 Years\" list at number 15. In 2012 VH1 ranked her third on their list of the \"100 Greatest Women in Music\". Beyoncé was the first female artist to be honored with the International Artist Award at the American Music Awards. She has also received the Legend Award at the 2008 World Music Awards and the Billboard Millennium Award at the 2011 Billboard Music Awards.']\n",
      "size of contexts: 5\n",
      "answers: {'text': ['the Protestant establishment'], 'answer_start': [297]}\n",
      "answers: {'text': ['Missy Elliott and Alicia Keys'], 'answer_start': [100]}\n",
      "answers: {'text': ['\"I curse the moment of my departure.\"'], 'answer_start': [500]}\n",
      "answers: {'text': [\"38 Rue de la Chaussée-d'Antin\"], 'answer_start': [88]}\n",
      "answers: {'text': ['64'], 'answer_start': [387]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_squad = squad.map(prepare_train_features, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ed466149ca31f82a\n",
      "Found cached dataset json (/home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 2/2 [00:00<00:00, 468.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# load my dataset\n",
    "from datasets import load_dataset\n",
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "\n",
    "my_datasets = load_dataset('json', data_files=data_files)\n",
    "\n",
    "# trim samples\n",
    "for split in my_datasets.keys():\n",
    "    my_datasets[split] = my_datasets[split].select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '593f14f960d971e294af884f0194b3a7',\n",
       " 'question': '舍本和誰的數據能推算出連星的恆星的質量？',\n",
       " 'paragraphs': [2018, 6952, 8264, 836],\n",
       " 'relevant': 836,\n",
       " 'answer': {'start': 108, 'text': '斯特魯維'}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56cf578daab44d1400b8908a',\n",
       " 'title': 'New_York_City',\n",
       " 'context': \"Major tourist destinations include Times Square; Broadway theater productions; the Empire State Building; the Statue of Liberty; Ellis Island; the United Nations Headquarters; museums such as the Metropolitan Museum of Art; greenspaces such as Central Park and Washington Square Park; Rockefeller Center; the Manhattan Chinatown; luxury shopping along Fifth and Madison Avenues; and events such as the Halloween Parade in Greenwich Village; the Macy's Thanksgiving Day Parade; the lighting of the Rockefeller Center Christmas Tree; the St. Patrick's Day parade; seasonal activities such as ice skating in Central Park in the wintertime; the Tribeca Film Festival; and free performances in Central Park at Summerstage. Major attractions in the boroughs outside Manhattan include Flushing Meadows-Corona Park and the Unisphere in Queens; the Bronx Zoo; Coney Island, Brooklyn; and the New York Botanical Garden in the Bronx. The New York Wheel, a 630-foot ferris wheel, was under construction at the northern shore of Staten Island in 2015, overlooking the Statue of Liberty, New York Harbor, and the Lower Manhattan skyline.\",\n",
       " 'question': 'What company sponsors the Thanksgiving Day parade?',\n",
       " 'answers': {'text': [\"Macy's\"], 'answer_start': [445]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load context.json\n",
    "import pandas as pd\n",
    "context_df = pd.read_json('./data/context.json')\n",
    "paragraph_ref_map = {}\n",
    "\n",
    "# enumerate context_df\n",
    "for i, row in context_df.iterrows():\n",
    "    # add context_df to context\n",
    "    paragraph_ref_map[i] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = my_datasets[\"train\"].column_names\n",
    "\n",
    "question_column_name = \"question\" \n",
    "answer_column_name = \"answer\"\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "args = {'doc_stride': 128}\n",
    "\n",
    "max_seq_length = min(384, tokenizer.model_max_length)\n",
    "\n",
    "def modified_prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    questions = [q.lstrip() for q in examples[question_column_name]]\n",
    "    contexts = []\n",
    "    for context_id in examples[\"relevant\"]:\n",
    "        contexts.append(paragraph_ref_map[context_id])\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions if pad_on_right else contexts,\n",
    "        contexts if pad_on_right else questions,\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        # TODO: need to change back to args.doc_stride\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        # TODO: need to change back to args.pad_to_max_length\n",
    "        padding=\"max_length\" if True else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        # check whether answers has start property\n",
    "        if not isinstance(answers[\"start\"], int):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"start\"]\n",
    "            end_char = start_char + len(answers[\"text\"])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 162.20ba/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess my dataset\n",
    "train_datasets = my_datasets[\"train\"]\n",
    "train_datasets = train_datasets.map(modified_prepare_train_features, batched=True, remove_columns=my_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    questions = [q.lstrip() for q in examples[question_column_name]]\n",
    "    contexts = []\n",
    "    for context_id in examples[\"relevant\"]:\n",
    "        contexts.append(paragraph_ref_map[context_id])\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions if pad_on_right else contexts,\n",
    "        contexts if pad_on_right else questions,\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if True else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "    # corresponding example_id and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00, 163.88ba/s]\n"
     ]
    }
   ],
   "source": [
    "eval_examples = my_datasets[\"validation\"]\n",
    "\n",
    "eval_dataset = eval_examples.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=not True,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing:\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=args.version_2_with_negative,\n",
    "        n_best_size=args.n_best_size,\n",
    "        max_answer_length=args.max_answer_length,\n",
    "        null_score_diff_threshold=args.null_score_diff_threshold,\n",
    "        output_dir=args.output_dir,\n",
    "        prefix=stage,\n",
    "        paragraph_ref_map=paragraph_ref_map,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'accelerate.accelerator' has no attribute 'main_process_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb Cell 45\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m accelerator\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m data_files \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m./data/train.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m./data/valid.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m accelerator\u001b[39m.\u001b[39;49mmain_process_first():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     eval_dataset \u001b[39m=\u001b[39m eval_examples\u001b[39m.\u001b[39mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         prepare_validation_features,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning tokenizer on validation dataset\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m raw_datasets \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m, data_files\u001b[39m=\u001b[39mdata_files)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'accelerate.accelerator' has no attribute 'main_process_first'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=2)\n",
    "\n",
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "with accelerator.main_process_first():\n",
    "    eval_dataset = eval_examples.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on validation dataset\",\n",
    "    )\n",
    "raw_datasets = load_dataset('json', data_files=data_files)\n",
    "eval_examples = raw_datasets[\"validation\"]\n",
    "prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5e7a923dd6e4ccb8730eb95230e0c908',\n",
       " 'question': '卡利創立的網際網路檔案館要到什麼時後才開放存取？',\n",
       " 'paragraphs': [8912, 7873, 8209, 7497]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/test.json') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固的女的一面或雙面蒙上拉緊的膜。鼓可以用手或鼓杵敲擊出聲。鼓在非洲的傳統音樂以及在現代音樂中是一種比較重要的樂器，有的樂隊完全由以鼓為主的打擊樂器組成。鼓除了作為樂器外，在古代許多文明中還用鼓來傳播信息。不同類型的鼓，如定音鼓等，均被調校至特定的音調中。更常見的是將不同類型的鼓或打擊樂器互相組合，以構成常於流行音樂出現的爵士鼓。鼓的演奏一般是用手敲擊鼓面，或是用一二隻鼓棒或鼓錘敲擊。由於鼓的觸覺特性及其易於使用，在音樂治療中常用到鼓，特別是手鼓。在許多傳統文化中，鼓有其象徵的意義，也常用在宗教儀式中。像在蒲隆地的卡央達鼓是王權的象徵，卡央達鼓也出現在1962至1966年間的蒲隆地國旗中。在流行音樂或爵士樂中，鼓常常是指由一組鼓及銅鈸組成的爵士鼓，演奏者稱為鼓手。鼓幾乎都有一個圓形的開口，鼓面拉緊後可以固定在上面。但鼓身的形狀就有很多的變化，西洋樂器的鼓，鼓身多半都是圓柱體，但定音鼓的鼓身則是碗形，有些鼓的鼓身則是截角圓錐或是二個接合的截角圓錐。中國、日本、韓國的鼓常常是中間略寬、上下略窄的圓柱體。最早的鼓是出現於西元前六千年的兩河文明。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "with open('./data/context.json') as f:\n",
    "    context = json.load(f)\n",
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['硬體是裝置在機箱內以做出個人電腦。系統軟體是儲存在硬體內，而系統軟體內含有韌體，例如基本輸入輸出系統以及作業系統，這些軟體使應用軟體可以提供使用者所需的功能。作業系統通常藉由匯流排與裝置溝通，這需要軟體提供驅動程式。輸入和輸出裝置通常裝在外部的主計算機機箱。輸入裝置使用戶能夠將資訊輸入到系統中，或控制其操作。大多數個人計算機的鼠標和鍵盤，但筆記本電腦的系統通常使用的鼠標觸摸板來代替。其他的輸入裝置包括網路攝像頭，麥克風，遊戲桿和圖像掃描儀。輸出裝置在人類可讀的形式顯示資訊。這種裝置可以包括印表機，音箱，顯示器或盲文壓花。計算機資料存儲，通常稱為存儲或內存，是指計算機部件和記錄媒介的保留數字資料。資料存儲是一個核心功能和計算機的基本組成部分。', '1996年，卡利創立了網際網路檔案館；同年10月開始收集儲存資料。不過，直到2001年開發了「時光機」前，這些資料都無法存取。1999年末擴展收集範圍。2012年8月，宣傳將在其現存的130萬檔案的下載選項中加入位元洪流。因為通過兩個檔案資料中心協調，這成為從該檔案館下載資料的最快方法。2013年11月6日，檔案館在里奇蒙德區的總部失火，損壞了許多裝置和一些附近的公寓，預計損失達到60萬美元。時光機是網際網路檔案館最重要的服務之一。時光機允許人們去搜尋和存取其網頁存檔。在一些國家和地區，時光機這個術語的使用已經非常普遍，「時光機」和「網際網路檔案館」甚至開始被當做同義詞使用。', '1994年10月，全球資訊網協會在麻省理工學院電腦科學實驗室成立，建立者是全球資訊網的發明者提姆·柏內茲-李。到了1994年底，全球網站數量依然相對較少，但是很多著名網站已經相當活躍，這些網站已經預示或者啟發了當今最流行的服務。透過網際網路連接，世界各地也建立了其他網站。這促進了協議和格式化的國際標準發展。柏內茲-李繼續參與指導全球資訊網標準的發展，例如標記式語言來組成網頁和他主張的語義網願景。全球資訊網透過易於使用和靈活的格式在網際網路上傳播資訊，因此對於網際網路的普及發揮了重要的作用。雖然這兩個術語有時被人們廣泛使用，但全球資訊網並不是網際網路的代名詞。全球資訊網是一個資訊空間，包含超連結文件和其他資源，並且由它們的URIs標識。網際網路和全球資訊網這兩個詞通常沒有多少區別。但是，兩者並不相同。網際網路是一個全球互相連接的電腦網路系統。相比之下，全球資訊網是透過超連結和URIs連接的全球收集檔案和其他資源。全球資訊網資源通常使用HTTP存取，這是許多網際網路通訊協議的其中之一。若要進入全球資訊網上一個網頁，或者其他網路資源的時候，通常需瀏覽器上鍵入你想存取網頁的統一資源定位符，或者通過超連結方式連結到那個網頁或網路資源。這之後的工作首先是URL的伺服器名部分，被名為域名系統的分布於全球的網際網路資料庫解析，並根據解析結果決定進入哪一個IP位址。接下來的步驟是為所要存取的網頁，向在那個IP位址工作的伺服器傳送一個HTTP請求。在通常情況下，HTML文字、圖片和構成該網頁的一切其他檔案很快會被逐一請求並行送回用戶。網路瀏覽器接下來的工作是把HTML、CSS和其他接受到的檔案所描述的內容，加上圖像、連結和其他必須的資源，顯示給用戶。這些就構成了你所看到的網頁。大多數的網頁自身包含有超連結指向其他相關網頁，可能還有下載、源文獻、定義和其他網路資源。像這樣通過超連結，把有用的相關資源組織在一起的集合，就形成了一個所謂的資訊的網。這個網在網際網路上被方便使用，就構成了最早在1990年代初提姆·柏內茲-李所說的全球資訊網。', '現代時期的開始代表了歐洲文藝復興的結束。確切的時間點會依據各個領域的界定而有所不同，比如說，一個歷史學家可能會將現代的開始放在1650年，而一個音樂家則可能會把時間放在音樂的浪漫時期結束之後，也就是大約1900年。當代史學者習慣把一戰爆發，作為近代的終結、現代的開端。同樣地，依據各個領域的不同觀點，現代時期可能會被界定為一直延伸到我們現在所處的時間，或者也有可能被認為結束於後現代主義的開始，可能在1960年代到1980年代初期之間的任何一點。如果是將現代定義為在後現代之前的話，那麼它可能特別指涉了現代主義。有的觀點則認為，後現代主義只不過是現代主義本身最晚一個時期的發展而已。']\n",
      "4\n",
      "['列寧的馬克思主義信念認為不能直接將當前的國家改造為共產主義社會，而必須首先進入社會主義時期，因此他主要關心如何將俄羅斯轉變為社會主義社會。為了能夠這樣做，他認為無產階級專政是必要的，以壓制資產階級和發展社會主義經濟。他界定社會主義為「生產資料由社會擁有的文明合作夥伴秩序」，且認為經濟體制必須擴張，直到創造後稀缺的充裕社會。為了實現這個目標，他認為在國家控制下將帶來俄羅斯經濟發展；而這也成為其核心關心的議題，更曾提出所有公民成為國家聘用的雇員。列寧對於社會主義的解釋為國家主義的中央集權和計畫經濟，生產與配給嚴格地受到控制。他認為全國各地的所有工人將會自發地聯合起來，實現國家經濟和政治的集中。列寧呼籲工人支配生產資料，且提到這並非由工人直接管理自身企業，而是所有企業的經營都在工人國家控制下。這導致列寧思想中出現兩種相互矛盾的主題，一方面廣泛的工人支配，另一方面則是中央集權、等級制度、高壓的國家機器。', '551年，百濟和新羅聯手攻打高句麗。高句麗從此失去具有重要戰略意義的朝鮮半島中部之肥沃的漢江流域。百濟新羅聯盟的主戰者百濟在對高句麗的戰爭幾乎精疲力盡。553年，新羅以幫百濟的名義出兵。但卻對百濟發動了攻勢，最後將整個漢江流域全部納入囊中。怒於新羅的背叛，百濟聖王第二年攻新羅西部以報復，但被新羅擒住，後被處死。朝鮮半島中部的戰爭，對朝鮮半島的格局產生了深遠的影響。新羅對百濟的攻擊使百濟成了朝鮮半島的最弱者。新羅由於竊取到了人口眾多，富饒的漢江流域，給其日後統一朝鮮半島奠定良好基礎。相反，高句麗卻因丟失漢江流域而國力大減。另外新羅獲得漢江流域後，疆域西界毗鄰黃海，使其可以和中原王朝直接貿易和建立外交。這樣新羅就不再依賴高句麗而是直接從中原王朝學到先進的文化與技術。新羅與中原王朝的直接溝通與聯盟最終使得在七世紀中期邀請唐軍赴朝鮮半島作戰，給高句麗帶來災難性的後果。', '同時也有的民主社會主義者，僅僅是認為史達林時期的所謂社會主義，是缺乏民主且官僚化的，他們認為應該捍衛自下而上的民主，因此成為民主的社會主義者。並且反對在資本主義體制內進行改革，而是希望徹底的自下而上的革命以取代。抱有此想法的，例如工人國際委員會。不像許多社會民主主義者受到第三條道路的影響，而願意以其他手段來解決貧窮問題，民主社會主義的革命路線和改革路線都堅持著福利國家的理想。革命路線者支持福利國家理想，不只因為這是一種達成社會主義目標的方式，也是作為一種在「革命尚未成功」之前救濟貧窮的過渡期政策，同時也能作為號召人民發動革命的動員手段。民主社會主義者仍然主張應該進行財富和權力的重新分派，並且將大多數的主要產業國有化，許多人也仍然支持計劃經濟，而這些都是社會民主主義者大多早已拋棄的概念。除此之外，許多民主社會主義者仍然保持馬克思主義的經濟學說，而社會民主主義則大多早已完全拋棄了馬克思的經濟學說。', '荷蘭叛亂的背景對於認識導致三十年戰爭原因是必需的。眾所周知，西班牙與荷蘭簽訂的十二年休戰條約將於1621年到期，當時整個歐洲意識到這點，西班牙帝國試圖再次征服荷蘭共和國。時任弗蘭德爾軍司令斯皮諾拉嘗試通過友好領土直抵荷蘭共和國，而行軍路線中唯一的敵對國家是普法茲選侯國，因此普法茲選侯國在歐洲具有相當重要的戰略意義遠遠超過了它自身的價值。這解釋了為什麼英格蘭國王詹姆斯一世於1612年為女兒伊麗莎白•斯圖亞特與普法茲選侯腓特烈五世的婚禮做準備，儘管當時的社會習俗和輿論認為公主該嫁給另一個王室。歐洲列強均想插手帝國事務，以獲取利益。它們分為兩派，其中哈布斯堡王朝集團由奧地利大公國、西班牙帝國與神聖羅馬帝國的天主教諸侯國組成，並得到羅馬教皇及波蘭立陶宛聯盟的支持；而另一方為反哈布斯堡王朝集團，由法蘭西王國、丹麥王國、瑞典帝國、荷蘭及德意志的新教諸侯國組成，並得到英格蘭王國、俄國的支持。法國雖是天主教國家，但是在首相黎塞留樞機主教的主導下，對於世俗政治的考量勝過了神學觀念，為削弱歐洲大陸上的頭號競爭對手哈布斯堡王朝，毅然支持新教國家陣營。']\n",
      "4\n",
      "['通常情況下，處於元音之間的單輔音常歸入後一音節，而元音之間若有兩個輔音，則一般分別歸入前後兩音節。但在古教會斯拉夫語等一些語言中，凡是能夠出現在詞首的輔音群都歸入後一音節。相反的情況也存在，在某些語言中，歸入後一音節的都是那些能夠出現在詞尾的輔音。在英語中，對於出現在元音之間的輔音應當歸入前一音節還是後一音節，是存有爭議的。一些語言學家就此提出了一個概念，稱這樣的詞語為「音節兩棲」的詞語。然而，威爾斯認為這不是一種有效的分析方法，他認為英語的音節區分僅僅存在一種情況。在英語中，輔音可同時被分析為一個音節的音節尾和下一個音節的音節首，這稱作「音節兩棲現象」。這一理論認為，類似英文「箭頭」這樣的單詞無法分成兩個彼此分離而又可單獨發音的單詞。但是威爾斯之後、弱化音節之前，那麼這個輔音或輔音群就是音節尾，而在其他的語音環境下則是音節首。', '現代時期的開始代表了歐洲文藝復興的結束。確切的時間點會依據各個領域的界定而有所不同，比如說，一個歷史學家可能會將現代的開始放在1650年，而一個音樂家則可能會把時間放在音樂的浪漫時期結束之後，也就是大約1900年。當代史學者習慣把一戰爆發，作為近代的終結、現代的開端。同樣地，依據各個領域的不同觀點，現代時期可能會被界定為一直延伸到我們現在所處的時間，或者也有可能被認為結束於後現代主義的開始，可能在1960年代到1980年代初期之間的任何一點。如果是將現代定義為在後現代之前的話，那麼它可能特別指涉了現代主義。有的觀點則認為，後現代主義只不過是現代主義本身最晚一個時期的發展而已。', '1899年3月15日光緒帝下聖旨授予天主教傳教士官銜：「分別教中品秩，如總教主或主教，其品位既與督撫相同，攝位司鐸、大司鐸，准其見司道，其餘司鐸，准其見府廳州縣各官。自督撫、司道、府廳州縣各官，亦要照品秩以禮相答」。1897年11月14日德軍強占膠州灣以後，12月18日德皇威廉二世在歡送弟弟海因里希親王率艦隊來華的《基爾演說》中有對中國要「用鐵拳打進去」的話。「義和」的提法並不是趙三多首創，在此之前，以「義和」命名的組織也曾零星地出現過，譬如1774年王倫叛亂中曾提及一個組織叫義和拳，1813年癸酉之變八卦教也有一個組織叫「義和門拳棒」。1860年代威縣為了對付捻軍，組建三支團練，其中就有一支名叫「義和團」。據目前所知，「義和拳」一名首見於1779年1月12日直隸總督周元理的奏摺中；該奏摺說：「奉旨：山東冠縣及直隸元城縣有民人楊姓等起立義和拳邪教，聚集多人之處，其所稱童姓，既有義和拳名色，為行路推車人之所知，斷難掩眾人之耳目」。', '練習神拳的人中逐漸出現了兩個著名的人，那就是朱紅燈和心誠和尚。1899年朱紅燈已經獲得了很大的名聲，附近貧苦民眾紛紛拜朱紅燈為師學習神拳。後來，神拳也改名為義和拳。但從大刀會到趙三多的義和拳，再到朱紅燈的義和拳，之間並沒有組織上的繼承關係。朱紅燈所領導的義和拳是真正意義上的義和拳，具備了1900年義和拳的所有特徵。從大刀會中借用了金鐘罩和「刀槍不入」的觀念，從趙三多的義和拳借用了名稱、扶清滅洋的口號等，然後加上了降神附體的巫術。戊戌政變後，光緒帝被慈禧幽禁在三面環水的中南海瀛台。守舊派大臣恐光緒怨於將來，紛紛慫恿慈禧太后廢掉光緒。慈禧太后遂依榮祿建議，於1900年1月24日以光緒帝名義頒詔，稱其不能誕育子嗣，乃立端郡王載漪之子，15歲的溥俊為大阿哥，史稱己亥建儲。']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    candidates = [context[paragraph_id] for paragraph_id in test[i][\"paragraphs\"]]\n",
    "    print(candidates)\n",
    "    print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test.json\n",
    "import json\n",
    "with open('./data/test.json') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "# load mc_result.json\n",
    "with open('./results/mc_result.json') as f:\n",
    "    mc_result = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test[0]: {'id': '5e7a923dd6e4ccb8730eb95230e0c908', 'question': '卡利創立的網際網路檔案館要到什麼時後才開放存取？', 'paragraphs': [8912, 7873, 8209, 7497]}\n",
      "mc_result[0]: {'id': '5e7a923dd6e4ccb8730eb95230e0c908', 'relevant': 7873}\n",
      "length of test: 2213\n",
      "length of mc_result: 2213\n"
     ]
    }
   ],
   "source": [
    "# every test entry and mc_result entry has id, and there is a corresponing relationship between them, match them first\n",
    "\n",
    "print(f\"test[0]: {test[0]}\")\n",
    "print(f\"mc_result[0]: {mc_result[0]}\")\n",
    "\n",
    "# check the length of test and mc_result\n",
    "print(f\"length of test: {len(test)}\")\n",
    "print(f\"length of mc_result: {len(mc_result)}\")\n",
    "\n",
    "mc_dict = {}\n",
    "for entry in mc_result:\n",
    "    mc_dict[entry[\"id\"]] = entry[\"relevant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_test[0]: {'id': '5e7a923dd6e4ccb8730eb95230e0c908', 'question': '卡利創立的網際網路檔案館要到什麼時後才開放存取？', 'paragraphs': [8912, 7873, 8209, 7497], 'relevant': 7873}\n",
      "merged_test[1]: {'id': 'a2e9cd802197b8f8dfbe235e2761f9ed', 'question': '哪個國家在歐洲具有重要的戰略意義甚至遠超過了其自身價值?', 'paragraphs': [744, 2457, 7423, 2674], 'relevant': 2674}\n",
      "merged_test[2]: {'id': 'c7c8a85b3f0006d44d86510a22193620', 'question': '目前所知「義和拳」這一個名詞最早於哪一年時出現?', 'paragraphs': [7393, 7497, 1015, 2305], 'relevant': 1015}\n",
      "merged_test[3]: {'id': '7f4f68726faed6b987e348340a9e6a61', 'question': '葉門是世界上經濟最落後的國家之一其主要倚賴什麼收入?', 'paragraphs': [5262, 7017, 6952, 49], 'relevant': 49}\n",
      "merged_test[4]: {'id': '89908ef5182021a9aec1472c5bbcbd8c', 'question': '北京地質學院博物館後來演變成哪一個博物館?', 'paragraphs': [1392, 5143, 1939, 2082], 'relevant': 1392}\n"
     ]
    }
   ],
   "source": [
    "merged_test = []\n",
    "for entry in test:\n",
    "    entry[\"relevant\"] = mc_dict[entry[\"id\"]]\n",
    "    merged_test.append(entry)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"merged_test[{i}]: {merged_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
