{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADL HW1 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12922050/miniconda3/envs/adl_hw1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset swag (/home/guest/r12922050/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n",
      "100%|██████████| 3/3 [00:00<00:00, 480.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 73546\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "swag = load_dataset('swag', 'regular')\n",
    "\n",
    "# take a look at swag dataset to see what it looks like\n",
    "swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>relevant</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593f14f960d971e294af884f0194b3a7</td>\n",
       "      <td>舍本和誰的數據能推算出連星的恆星的質量？</td>\n",
       "      <td>[2018, 6952, 8264, 836]</td>\n",
       "      <td>836</td>\n",
       "      <td>{'text': '斯特魯維', 'start': 108}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acd5d763ec4c250f9a11eac1412d6814</td>\n",
       "      <td>在關西鎮以什麼方言為主？</td>\n",
       "      <td>[1716, 8318, 4070, 7571]</td>\n",
       "      <td>8318</td>\n",
       "      <td>{'text': '四縣腔客家話', 'start': 306}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5670000714a658c5e52658e22e6985f7</td>\n",
       "      <td>「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?</td>\n",
       "      <td>[6043, 5950, 2548, 5806]</td>\n",
       "      <td>5806</td>\n",
       "      <td>{'text': '王翦', 'start': 46}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f71493751ccfce42aae7a99ed3f20dbb</td>\n",
       "      <td>《方法論》的作者是誰?</td>\n",
       "      <td>[5443, 6843, 8584, 4350]</td>\n",
       "      <td>6843</td>\n",
       "      <td>{'text': '阿基米德', 'start': 63}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56d547357d11a8938197072d82aa126c</td>\n",
       "      <td>環繞速度又為哪種速度的別稱?</td>\n",
       "      <td>[1859, 7590, 3116, 8989]</td>\n",
       "      <td>7590</td>\n",
       "      <td>{'text': '第一宇宙速度', 'start': 69}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21709</th>\n",
       "      <td>0ba8689b6665bb8d700807e586244018</td>\n",
       "      <td>又稱爲虎斑的為什麼?</td>\n",
       "      <td>[5662, 68, 7394, 92]</td>\n",
       "      <td>7394</td>\n",
       "      <td>{'text': '土衛二', 'start': 213}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21710</th>\n",
       "      <td>9405202eb2a8391136a5905987ebab26</td>\n",
       "      <td>愛爾蘭王國的王位在哪一年正式獲得神權認可?</td>\n",
       "      <td>[641, 6490, 3870, 2146]</td>\n",
       "      <td>3870</td>\n",
       "      <td>{'text': '1555年', 'start': 47}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21711</th>\n",
       "      <td>8dda88faa2c6b074a48b66cb74662727</td>\n",
       "      <td>《自題小像》是誰的作品?</td>\n",
       "      <td>[1991, 3309, 4956, 6311]</td>\n",
       "      <td>1991</td>\n",
       "      <td>{'text': '魯迅', 'start': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21712</th>\n",
       "      <td>92484786f021da4ed4dcf952401a7402</td>\n",
       "      <td>日蝕和月蝕在何時代就可以被算出來了？</td>\n",
       "      <td>[2745, 5391, 928, 4386]</td>\n",
       "      <td>928</td>\n",
       "      <td>{'text': '古希臘', 'start': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21713</th>\n",
       "      <td>7faf3194e307cde1832a21c2d999fa17</td>\n",
       "      <td>牛津大學在2004年與哪家公司合作進行大規模的數位化系統更新項目？</td>\n",
       "      <td>[1502, 8215, 1156, 8824]</td>\n",
       "      <td>1156</td>\n",
       "      <td>{'text': '谷歌', 'start': 235}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21714 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0      593f14f960d971e294af884f0194b3a7   \n",
       "1      acd5d763ec4c250f9a11eac1412d6814   \n",
       "2      5670000714a658c5e52658e22e6985f7   \n",
       "3      f71493751ccfce42aae7a99ed3f20dbb   \n",
       "4      56d547357d11a8938197072d82aa126c   \n",
       "...                                 ...   \n",
       "21709  0ba8689b6665bb8d700807e586244018   \n",
       "21710  9405202eb2a8391136a5905987ebab26   \n",
       "21711  8dda88faa2c6b074a48b66cb74662727   \n",
       "21712  92484786f021da4ed4dcf952401a7402   \n",
       "21713  7faf3194e307cde1832a21c2d999fa17   \n",
       "\n",
       "                                        question                paragraphs  \\\n",
       "0                           舍本和誰的數據能推算出連星的恆星的質量？   [2018, 6952, 8264, 836]   \n",
       "1                                   在關西鎮以什麼方言為主？  [1716, 8318, 4070, 7571]   \n",
       "2      「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?  [6043, 5950, 2548, 5806]   \n",
       "3                                    《方法論》的作者是誰?  [5443, 6843, 8584, 4350]   \n",
       "4                                 環繞速度又為哪種速度的別稱?  [1859, 7590, 3116, 8989]   \n",
       "...                                          ...                       ...   \n",
       "21709                                 又稱爲虎斑的為什麼?      [5662, 68, 7394, 92]   \n",
       "21710                      愛爾蘭王國的王位在哪一年正式獲得神權認可?   [641, 6490, 3870, 2146]   \n",
       "21711                               《自題小像》是誰的作品?  [1991, 3309, 4956, 6311]   \n",
       "21712                         日蝕和月蝕在何時代就可以被算出來了？   [2745, 5391, 928, 4386]   \n",
       "21713          牛津大學在2004年與哪家公司合作進行大規模的數位化系統更新項目？  [1502, 8215, 1156, 8824]   \n",
       "\n",
       "       relevant                            answer  \n",
       "0           836    {'text': '斯特魯維', 'start': 108}  \n",
       "1          8318  {'text': '四縣腔客家話', 'start': 306}  \n",
       "2          5806       {'text': '王翦', 'start': 46}  \n",
       "3          6843     {'text': '阿基米德', 'start': 63}  \n",
       "4          7590   {'text': '第一宇宙速度', 'start': 69}  \n",
       "...         ...                               ...  \n",
       "21709      7394     {'text': '土衛二', 'start': 213}  \n",
       "21710      3870    {'text': '1555年', 'start': 47}  \n",
       "21711      1991        {'text': '魯迅', 'start': 0}  \n",
       "21712       928       {'text': '古希臘', 'start': 0}  \n",
       "21713      1156      {'text': '谷歌', 'start': 235}  \n",
       "\n",
       "[21714 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfer my dataset to data format like swag\n",
    "\n",
    "import pandas as pd\n",
    "train_data = pd.read_json('./data/train.json')\n",
    "\n",
    "# take a look at train_data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9010</th>\n",
       "      <td>因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9013 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...\n",
       "1     這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...\n",
       "2     處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...\n",
       "3     福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...\n",
       "4     盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...\n",
       "...                                                 ...\n",
       "9008  在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...\n",
       "9009  雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...\n",
       "9010  因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...\n",
       "9011  因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...\n",
       "9012  針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...\n",
       "\n",
       "[9013 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loda context.json\n",
    "context_data = pd.read_json('./data/context.json')\n",
    "\n",
    "# take a look at context_data\n",
    "context_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            593f14f960d971e294af884f0194b3a7\n",
       "question                  舍本和誰的數據能推算出連星的恆星的質量？\n",
       "paragraphs             [2018, 6952, 8264, 836]\n",
       "relevant                                   836\n",
       "answer          {'text': '斯特魯維', 'start': 108}\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first row of train_data\n",
    "train_data.iloc[0]\n",
    "# paragraphs are 4 context ids that represent 4 possible choices, and the id can refer to context.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first row of swag\n",
    "swag['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>relevant</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593f14f960d971e294af884f0194b3a7</td>\n",
       "      <td>舍本和誰的數據能推算出連星的恆星的質量？</td>\n",
       "      <td>[2018, 6952, 8264, 836]</td>\n",
       "      <td>836</td>\n",
       "      <td>{'text': '斯特魯維', 'start': 108}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acd5d763ec4c250f9a11eac1412d6814</td>\n",
       "      <td>在關西鎮以什麼方言為主？</td>\n",
       "      <td>[1716, 8318, 4070, 7571]</td>\n",
       "      <td>8318</td>\n",
       "      <td>{'text': '四縣腔客家話', 'start': 306}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5670000714a658c5e52658e22e6985f7</td>\n",
       "      <td>「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?</td>\n",
       "      <td>[6043, 5950, 2548, 5806]</td>\n",
       "      <td>5806</td>\n",
       "      <td>{'text': '王翦', 'start': 46}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f71493751ccfce42aae7a99ed3f20dbb</td>\n",
       "      <td>《方法論》的作者是誰?</td>\n",
       "      <td>[5443, 6843, 8584, 4350]</td>\n",
       "      <td>6843</td>\n",
       "      <td>{'text': '阿基米德', 'start': 63}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56d547357d11a8938197072d82aa126c</td>\n",
       "      <td>環繞速度又為哪種速度的別稱?</td>\n",
       "      <td>[1859, 7590, 3116, 8989]</td>\n",
       "      <td>7590</td>\n",
       "      <td>{'text': '第一宇宙速度', 'start': 69}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  593f14f960d971e294af884f0194b3a7   \n",
       "1  acd5d763ec4c250f9a11eac1412d6814   \n",
       "2  5670000714a658c5e52658e22e6985f7   \n",
       "3  f71493751ccfce42aae7a99ed3f20dbb   \n",
       "4  56d547357d11a8938197072d82aa126c   \n",
       "\n",
       "                                    question                paragraphs  \\\n",
       "0                       舍本和誰的數據能推算出連星的恆星的質量？   [2018, 6952, 8264, 836]   \n",
       "1                               在關西鎮以什麼方言為主？  [1716, 8318, 4070, 7571]   \n",
       "2  「有錫兵，天下爭。無錫寧，天下清。」指的是何人攻破蘭陵後，率軍駐無錫錫山時的上書?  [6043, 5950, 2548, 5806]   \n",
       "3                                《方法論》的作者是誰?  [5443, 6843, 8584, 4350]   \n",
       "4                             環繞速度又為哪種速度的別稱?  [1859, 7590, 3116, 8989]   \n",
       "\n",
       "   relevant                            answer  \n",
       "0       836    {'text': '斯特魯維', 'start': 108}  \n",
       "1      8318  {'text': '四縣腔客家話', 'start': 306}  \n",
       "2      5806       {'text': '王翦', 'start': 46}  \n",
       "3      6843     {'text': '阿基米德', 'start': 63}  \n",
       "4      7590   {'text': '第一宇宙速度', 'start': 69}  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 5 rows of train_data\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "- for train and valid, use `load_dataset`\n",
    "- for context, it's a normal json file, use `pd.read_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ed466149ca31f82a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 3276.80it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 253.96it/s]\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 716.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'paragraphs', 'relevant', 'answer'],\n",
       "        num_rows: 21714\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'paragraphs', 'relevant', 'answer'],\n",
       "        num_rows: 3009\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "\n",
    "my_datasets = load_dataset('json', data_files=data_files)\n",
    "my_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context.json\n",
    "- after reading it, rename it to paragraph_ref_map for index reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9010</th>\n",
       "      <td>因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9013 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固...\n",
       "1     這次出售的贖罪券很特別，是全大赦贖罪券，可以贖買過去所犯的罪攢下來的所有刑罰，將購買者重新恢...\n",
       "2     處在千年古都的西安交大校園少不了和歷史千絲萬縷的聯繫。興慶校區所處位置為唐長安城內的道政、常...\n",
       "3     福臨於1651年親政後，他的母親昭聖慈壽皇太后安排兒子娶她的侄女額爾德尼奔巴，但福臨於165...\n",
       "4     盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡...\n",
       "...                                                 ...\n",
       "9008  在握網球拍的方式中，東方式正手握拍法如同我們與對方握手的姿勢一樣，先把手平貼在拍面上，保持手...\n",
       "9009  雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理...\n",
       "9010  因岳陽地處長江要道，自古被貶的遷客騷人多匯聚與此然後轉道，溯長江而上可至巴、蜀一帶，由湘江而...\n",
       "9011  因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，...\n",
       "9012  針對天體攝影的天體攝影術誕生於1840年，當時約翰·威廉·德雷伯使用銀版照相法對月球進行攝影...\n",
       "\n",
       "[9013 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read context.json\n",
    "context_df = pd.read_json('./data/context.json')\n",
    "context_df\n",
    "\n",
    "paragraph_ref_map = {}\n",
    "# enumerate context_df\n",
    "for i, row in context_df.iterrows():\n",
    "    # add context_df to context\n",
    "    paragraph_ref_map[i] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_ref_map = {}\n",
    "# enumerate context_df\n",
    "for i, row in context_df.iterrows():\n",
    "    # add context_df to context\n",
    "    paragraph_ref_map[i] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鼓是一種打擊樂器，也是一種通訊工具，非洲某些部落用以傳達信息，中國古代軍隊用以發號施令。堅固的女的一面或雙面蒙上拉緊的膜。鼓可以用手或鼓杵敲擊出聲。鼓在非洲的傳統音樂以及在現代音樂中是一種比較重要的樂器，有的樂隊完全由以鼓為主的打擊樂器組成。鼓除了作為樂器外，在古代許多文明中還用鼓來傳播信息。不同類型的鼓，如定音鼓等，均被調校至特定的音調中。更常見的是將不同類型的鼓或打擊樂器互相組合，以構成常於流行音樂出現的爵士鼓。鼓的演奏一般是用手敲擊鼓面，或是用一二隻鼓棒或鼓錘敲擊。由於鼓的觸覺特性及其易於使用，在音樂治療中常用到鼓，特別是手鼓。在許多傳統文化中，鼓有其象徵的意義，也常用在宗教儀式中。像在蒲隆地的卡央達鼓是王權的象徵，卡央達鼓也出現在1962至1966年間的蒲隆地國旗中。在流行音樂或爵士樂中，鼓常常是指由一組鼓及銅鈸組成的爵士鼓，演奏者稱為鼓手。鼓幾乎都有一個圓形的開口，鼓面拉緊後可以固定在上面。但鼓身的形狀就有很多的變化，西洋樂器的鼓，鼓身多半都是圓柱體，但定音鼓的鼓身則是碗形，有些鼓的鼓身則是截角圓錐或是二個接合的截角圓錐。中國、日本、韓國的鼓常常是中間略寬、上下略窄的圓柱體。最早的鼓是出現於西元前六千年的兩河文明。\n",
      "盧克萊修生於共和國末期，唯一的傳世之作《物性論》共六卷，每卷千餘行，是一部哲理詩。全詩著重闡述伊壁鳩魯的哲學思想和德謨克利特的原子論，表示物質不滅、凡人不必懼怕死亡的生活觀。盧克萊修是古羅馬文學史上著名的智者，維吉爾曾稱羨慕他知曉事物的起因，是個幸福的人。卡圖魯斯生於義大利北部維羅那一個富有的家庭，經常出入羅馬上流社會，是黃金時代成就最高的抒情詩人。他是堅定不移的共和派，曾公然反對過愷撒，曾創作過許多辛辣的諷刺短詩。卡圖魯斯的詩作現存116首，他善用警句式的語言表達濃鬱熱烈、複雜微妙的感情。他的抒情詩對後世歐洲許多偉大詩人都產生過影響。賀拉斯出生於拍賣商家庭，是和卡圖魯斯齊名的抒情詩人。他幼年受過良好的教育，通曉拉丁語和希臘語，能誦荷馬史詩原文，併到雅典學過哲學。他的代表作品包括《長短句集》17首和《閒談集》18首。前者表明作者反對內戰，幻想黃金時代到來的思想；後者則諷刺羅馬社會的惡習。但賀拉斯最著名的作品是後期的《歌集》和《詩藝》。賀拉斯的抒情詩改造了希臘抒情詩的格律，構思巧妙，語言優美，優雅莊重，以有意、愛情、詩藝為題，融哲理和感情於一路，不少人競相模仿。《詩藝》則是古羅馬時期文藝理論上的最高成就，被古典主義文學視為經典。\n",
      "雖然認識到了狹義相對論需要推廣為廣義相對論，並確立了兩條基本原理，愛因斯坦仍然為探索這一新理論研究了八年之久。這期間他面臨的一個主要問題是缺乏有效的數學工具，直到1913年他在德國數學家馬塞爾·格羅斯曼的幫助下發表了一篇突破性的論文：《廣義相對論和重力理論綱要》，題目標註了物理部分作者為愛因斯坦，數學部分作者為格羅斯曼。這篇論文中原來單一的牛頓純量重力場被一個具有十個分量的四階對稱黎曼張量重力場替代，從此物理學中的時空不再是平直的，而成為了在全局上具有曲率但在局部平直的黎曼流形。1914年，愛因斯坦發表了《廣義相對論正式基礎》，其中他得到了廣義相對論中描述粒子運動的方程式：測地線方程式，並籍此推導了重力場中的光線偏折和重力紅移的結果。1915年11月，愛因斯坦發表了他最終推導出重力場方程式的四篇論文，其中《用廣義相對論解釋水星近日點運動》證明了廣義相對論能夠解釋自1859年以來困擾天文學家的水星的反常進動現象，而《重力場方程式》則正式給出了描述重力場和物質交互作用的愛因斯坦重力場方程式。\n"
     ]
    }
   ],
   "source": [
    "# check some ids\n",
    "print(paragraph_ref_map[0])\n",
    "print(paragraph_ref_map[4])\n",
    "print(paragraph_ref_map[9009])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: swag/regular\n",
      "Found cached dataset swag (/home/guest/r12922050/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n",
      "100%|██████████| 3/3 [00:00<00:00, 583.03it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"swag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2118.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 1]\n",
      "first sentences: [['Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession walk down the street holding small horn brass instruments.'], ['A drum line passes by walking down the street playing their instruments.', 'A drum line passes by walking down the street playing their instruments.', 'A drum line passes by walking down the street playing their instruments.', 'A drum line passes by walking down the street playing their instruments.'], ['A group of members in green uniforms walks waving flags.', 'A group of members in green uniforms walks waving flags.', 'A group of members in green uniforms walks waving flags.', 'A group of members in green uniforms walks waving flags.']]\n",
      "second sentences: [['A drum line passes by walking down the street playing their instruments.', 'A drum line has heard approaching them.', \"A drum line arrives and they're outside dancing and asleep.\", 'A drum line turns the lead singer watches the performance.'], ['Members of the procession are playing ping pong and celebrating one left each in quick.', 'Members of the procession wait slowly towards the cadets.', 'Members of the procession continues to play as well along the crowd along with the band being interviewed.', 'Members of the procession continue to play marching, interspersed.'], ['Members of the procession pay the other coaches to cheer as people this chatter dips in lawn sheets.', 'Members of the procession walk down the street holding small horn brass instruments.', 'Members of the procession is seen in the background.', 'Members of the procession are talking a couple of people playing a game of tug of war.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1992.54ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2]\n",
      "first sentences: [['Students lower their eyes nervously.', 'Students lower their eyes nervously.', 'Students lower their eyes nervously.', 'Students lower their eyes nervously.'], ['He rides the motorcycle down the hall and into the elevator.', 'He rides the motorcycle down the hall and into the elevator.', 'He rides the motorcycle down the hall and into the elevator.', 'He rides the motorcycle down the hall and into the elevator.'], ['The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.', 'The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.', 'The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.', 'The motorcyclist gets out of bed and prepares to leave by bathing using the bathroom and reading the newspaper.']]\n",
      "second sentences: [['She pats her shoulder, then saunters toward someone.', 'She turns with two students.', 'She walks slowly towards someone.', 'She wheels around as her dog thunders out.'], ['He looks at a mirror in the mirror as he watches someone walk through a door.', \"He stops, listening to a cup of coffee with the seated woman, who's standing.\", 'He exits the building and rides the motorcycle into a casino where he performs several tricks as people watch.', \"He pulls the bag out of his pocket and hands it to someone's grandma.\"], ['He shoots a look at her.', 'He makes his way past it and peers out a window.', 'He rides the motorcycle down the hall and into the elevator.', 'He sits on the ground beside her pants, clinging by a flannel wool.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2114.06ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1]\n",
      "first sentences: [['A person shows the bottom of a large dust mop.', 'A person shows the bottom of a large dust mop.', 'A person shows the bottom of a large dust mop.', 'A person shows the bottom of a large dust mop.'], ['The camera move and we see a man next to the lady.', 'The camera move and we see a man next to the lady.', 'The camera move and we see a man next to the lady.', 'The camera move and we see a man next to the lady.'], ['The lady stops and adjusts the camera.', 'The lady stops and adjusts the camera.', 'The lady stops and adjusts the camera.', 'The lady stops and adjusts the camera.']]\n",
      "second sentences: [['Then, the person places the scrub on the floor then cleans the soap evenly.', 'Then, the person cleans a gym with the large dust mop.', 'Then, the person stops and spray a car with some spray paint.', 'Then, the person places the bucket and a bucket.'], ['the lady stands on a bench in the bar.', 'the lady put a piece down on the table.', 'the lady fixes her hair in the camera.', 'the lady completes a picture and shows a standing room.'], ['the camera move and we see a man going down the street.', 'the camera move and we see a lady kick a scene.', 'the camera move and we see a man next to the lady.', 'the camera move and we see a closeup of her hair before sitting down again.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# trim samples\n",
    "for split in raw_datasets.keys():\n",
    "    raw_datasets[split] = raw_datasets[split].select(range(3))\n",
    "\n",
    "if raw_datasets[\"train\"] is not None:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "else:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "\n",
    "ending_names = [f\"ending{i}\" for i in range(4)]\n",
    "context_name = \"sent1\"\n",
    "question_header_name = \"sent2\"\n",
    "label_column_name = \"label\" if \"label\" in column_names else \"labels\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[context_name]]\n",
    "    # size of first_sentences\n",
    "    question_headers = examples[question_header_name]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "    labels = examples[label_column_name]\n",
    "    print(labels)\n",
    "    print(f\"first sentences: {first_sentences}\")\n",
    "    print(f\"second sentences: {second_sentences}\")\n",
    "    # Flatten out\n",
    "    first_sentences = list(chain(*first_sentences))\n",
    "    second_sentences = list(chain(*second_sentences))\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '593f14f960d971e294af884f0194b3a7',\n",
       " 'question': '舍本和誰的數據能推算出連星的恆星的質量？',\n",
       " 'paragraphs': [2018, 6952, 8264, 836],\n",
       " 'relevant': 836,\n",
       " 'answer': {'start': 108, 'text': '斯特魯維'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ed466149ca31f82a\n",
      "Found cached dataset json (/home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 2/2 [00:00<00:00, 410.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "\n",
    "my_datasets = load_dataset('json', data_files=data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim samples\n",
    "for split in my_datasets.keys():\n",
    "    my_datasets[split] = my_datasets[split].select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 10.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 624/624 [00:00<00:00, 1.19MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 37.3MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 269k/269k [00:00<00:00, 497kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator(gradient_accumulation_steps=2)\n",
    "from itertools import chain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "if my_datasets[\"train\"] is not None:\n",
    "    column_names = my_datasets[\"train\"].column_names\n",
    "else:\n",
    "    column_names = my_datasets[\"validation\"].column_names\n",
    "\n",
    "context_name = \"question\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[context_name]]\n",
    "    second_sentences = []\n",
    "    labels = []\n",
    "    for i, paragraphs in enumerate(examples['paragraphs']):\n",
    "        correct_paragraph = examples['relevant'][i]\n",
    "        possible_ans = []\n",
    "        for ans_idx, paragraph in enumerate(paragraphs):\n",
    "            if paragraph == correct_paragraph:\n",
    "                labels.append(ans_idx)\n",
    "            possible_ans.append(paragraph_ref_map[paragraph])\n",
    "        second_sentences.append(possible_ans)\n",
    "\n",
    "    # Flatten out\n",
    "    first_sentences = list(chain(*first_sentences))\n",
    "    second_sentences = list(chain(*second_sentences))\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        max_length=args.max_seq_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # Un-flatten\n",
    "    tokenized_inputs = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with accelerator.main_process_first():\n",
    "    processed_datasets = my_datasets.map(\n",
    "        preprocess_function, batched=True, remove_columns=my_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 13428 of the training set: 0.\n",
      "{'id': 'a6407580654244d6efdb31db964726ce', 'question': '印度全境炎熱，大部分屬於何種氣候?', 'paragraphs': [1812, 4272, 4849, 3443], 'relevant': 1812, 'answer': {'start': 744, 'text': '熱帶季風'}}\n"
     ]
    }
   ],
   "source": [
    "index = 13428\n",
    "print(f\"Sample {index} of the training set: {processed_datasets['train'][index]['labels']}.\")\n",
    "print(my_datasets[\"train\"][13428])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12922050/miniconda3/envs/adl-hw1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.43kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 90.8kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 626kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.48MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/guest/r12922050/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "\n",
    "# trim samples\n",
    "for split in squad.keys():\n",
    "    squad[split] = squad[split].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733f18ad058e614000b6644',\n",
       " 'title': 'Montana',\n",
       " 'context': 'The name Montana comes from the Spanish word Montaña, meaning \"mountain\", or more broadly, \"mountainous country\". Montaña del Norte was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was successfully changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained that Montana had \"no meaning\". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given that most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as Shoshone were suggested, but it was eventually decided that the Committee on Territories could name it whatever they wanted, so the original name of Montana was adopted.',\n",
       " 'question': 'What did the Spanish call this region?',\n",
       " 'answers': {'text': ['Montaña del Norte'], 'answer_start': [114]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: answer_start 的 value 代表的是 index，因此像 155 就代表是第 155 個字\n",
    "\n",
    "- answers: the starting location of the answer token and the answer text.\n",
    "- context: background information from which the model needs to extract the answer.\n",
    "- question: the question a model should answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = squad[\"train\"].column_names\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "args = {'doc_stride': 128}\n",
    "\n",
    "max_seq_length = min(384, tokenizer.model_max_length)\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    print(f\"questions: {examples[question_column_name]}\")\n",
    "    print(f\"contexts: {examples[context_column_name]}\")\n",
    "    # check size of contexts\n",
    "    print(f\"size of contexts: {len(examples[context_column_name])}\")\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if True else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        print(f\"answers: {answers}\")\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`truncation=\"only_second\"` means only the second sequence will be truncated to fit the maximum length specified by `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 114.87ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions: ['What did the Spanish call this region?', 'What film did Beyonce appear in with Mike Myers?', \"What is the annual budget of Notre Dame's LaFortune Center?\", \"What publication's partial reprinting gave the book wide public exposure?\", 'What city saw the largest growth?']\n",
      "contexts: ['The name Montana comes from the Spanish word Montaña, meaning \"mountain\", or more broadly, \"mountainous country\". Montaña del Norte was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was successfully changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained that Montana had \"no meaning\". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given that most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as Shoshone were suggested, but it was eventually decided that the Committee on Territories could name it whatever they wanted, so the original name of Montana was adopted.', 'In July 2002, Beyoncé continued her acting career playing Foxxy Cleopatra alongside Mike Myers in the comedy film, Austin Powers in Goldmember, which spent its first weekend atop the US box office and grossed $73 million. Beyoncé released \"Work It Out\" as the lead single from its soundtrack album which entered the top ten in the UK, Norway, and Belgium. In 2003, Beyoncé starred opposite Cuba Gooding, Jr., in the musical comedy The Fighting Temptations as Lilly, a single mother whom Gooding\\'s character falls in love with. The film received mixed reviews from critics but grossed $30 million in the U.S. Beyoncé released \"Fighting Temptation\" as the lead single from the film\\'s soundtrack album, with Missy Elliott, MC Lyte, and Free which was also used to promote the film. Another of Beyoncé\\'s contributions to the soundtrack, \"Summertime\", fared better on the US charts.', 'A Science Hall was built in 1883 under the direction of Fr. Zahm, but in 1950 it was converted to a student union building and named LaFortune Center, after Joseph LaFortune, an oil executive from Tulsa, Oklahoma. Commonly known as \"LaFortune\" or \"LaFun,\" it is a 4-story building of 83,000 square feet that provides the Notre Dame community with a meeting place for social, recreational, cultural, and educational activities. LaFortune employs 35 part-time student staff and 29 full-time non-student staff and has an annual budget of $1.2 million. Many businesses, services, and divisions of The Office of Student Affairs are found within. The building also houses restaurants from national restaurant chains.', 'Ultimately, Lee spent over two and a half years writing To Kill a Mockingbird. The book was published on July 11, 1960. After rejecting the \"Watchman\" title, it was initially re-titled Atticus, but Lee renamed it \"To Kill a Mockingbird\" to reflect that the story went beyond just a character portrait. The editorial team at Lippincott warned Lee that she would probably sell only several thousand copies. In 1964, Lee recalled her hopes for the book when she said, \"I never expected any sort of success with \\'Mockingbird.\\' ... I was hoping for a quick and merciful death at the hands of the reviewers but, at the same time, I sort of hoped someone would like it enough to give me encouragement. Public encouragement. I hoped for a little, as I said, but I got rather a whole lot, and in some ways this was just about as frightening as the quick, merciful death I\\'d expected.\" Instead of a \"quick and merciful death\", Reader\\'s Digest Condensed Books chose the book for reprinting in part, which gave it a wide readership immediately. Since the original publication, the book has never been out of print.', \"The United States Census Bureau estimates that the population of Montana was 1,032,949 on July 1, 2015, a 4.40% increase since the 2010 United States Census. The 2010 census put Montana's population at 989,415 which is an increase of 43,534 people, or 4.40 percent, since 2010. During the first decade of the new century, growth was mainly concentrated in Montana's seven largest counties, with the highest percentage growth in Gallatin County, which saw a 32 percent increase in its population from 2000-2010. The city seeing the largest percentage growth was Kalispell with 40.1 percent, and the city with the largest increase in actual residents was Billings with an increase in population of 14,323 from 2000-2010.\"]\n",
      "size of contexts: 5\n",
      "answers: {'text': ['Montaña del Norte'], 'answer_start': [114]}\n",
      "answers: {'text': ['Austin Powers in Goldmember'], 'answer_start': [115]}\n",
      "answers: {'text': ['$1.2 million'], 'answer_start': [535]}\n",
      "answers: {'text': [\"Reader's Digest Condensed Books\"], 'answer_start': [917]}\n",
      "answers: {'text': ['Kalispell'], 'answer_start': [561]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 150.80ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions: ['Catholic people identified with Notre Dame, what religious group did people feel Yale represented?', 'Who did Beyoncé tour with for the Verizon Lades First Tour?', 'What were the words Chopin wrote to a friend when he was alone and homesick?', 'What address did Frédéric live at during his stay in Paris?', 'How many certifications did RIAA give Beyoncé?']\n",
      "contexts: [\"The success of its football team made Notre Dame a household name. The success of Note Dame reflected rising status of Irish Americans and Catholics in the 1920s. Catholics rallied up around the team and listen to the games on the radio, especially when it knocked off the schools that symbolized the Protestant establishment in America — Harvard, Yale, Princeton, and Army. Yet this role as high-profile flagship institution of Catholicism made it an easy target of anti-Catholicism. The most remarkable episode of violence was the clash between Notre Dame students and the Ku Klux Klan in 1924. Nativism and anti-Catholicism, especially when directed towards immigrants, were cornerstones of the KKK's rhetoric, and Notre Dame was seen as a symbol of the threat posed by the Catholic Church. The Klan decided to have a week-long Klavern in South Bend. Clashes with the student body started on March 17, when students, aware of the anti-Catholic animosity, blocked the Klansmen from descending from their trains in the South Bend station and ripped the KKK clothes and regalia. On May 19 thousands of students massed downtown protesting the Klavern, and only the arrival of college president Fr. Matthew Walsh prevented any further clashes. The next day, football coach Knute Rockne spoke at a campus rally and implored the students to obey the college president and refrain from further violence. A few days later the Klavern broke up, but the hostility shown by the students was an omen and a contribution to the downfall of the KKK in Indiana.\", 'In November 2003, she embarked on the Dangerously in Love Tour in Europe and later toured alongside Missy Elliott and Alicia Keys for the Verizon Ladies First Tour in North America. On February 1, 2004, Beyoncé performed the American national anthem at Super Bowl XXXVIII, at the Reliant Stadium in Houston, Texas. After the release of Dangerously in Love, Beyoncé had planned to produce a follow-up album using several of the left-over tracks. However, this was put on hold so she could concentrate on recording Destiny Fulfilled, the final studio album by Destiny\\'s Child. Released on November 15, 2004, in the US and peaking at number two on the Billboard 200, Destiny Fulfilled included the singles \"Lose My Breath\" and \"Soldier\", which reached the top five on the Billboard Hot 100 chart. Destiny\\'s Child embarked on a worldwide concert tour, Destiny Fulfilled... and Lovin\\' It and during the last stop of their European tour, in Barcelona on June 11, 2005, Rowland announced that Destiny\\'s Child would disband following the North American leg of the tour. The group released their first compilation album Number 1\\'s on October 25, 2005, in the US and accepted a star on the Hollywood Walk of Fame in March 2006.', 'Chopin\\'s successes as a composer and performer opened the door to western Europe for him, and on 2 November 1830, he set out, in the words of Zdzisław Jachimecki, \"into the wide world, with no very clearly defined aim, forever.\" With Woyciechowski, he headed for Austria, intending to go on to Italy. Later that month, in Warsaw, the November 1830 Uprising broke out, and Woyciechowski returned to Poland to enlist. Chopin, now alone in Vienna, was nostalgic for his homeland, and wrote to a friend, \"I curse the moment of my departure.\" When in September 1831 he learned, while travelling from Vienna to Paris, that the uprising had been crushed, he expressed his anguish in the pages of his private journal: \"Oh God! ... You are there, and yet you do not take vengeance!\" Jachimecki ascribes to these events the composer\\'s maturing \"into an inspired national bard who intuited the past, present and future of his native Poland.\"', \"The two became friends, and for many years lived in close proximity in Paris, Chopin at 38 Rue de la Chaussée-d'Antin, and Liszt at the Hôtel de France on the Rue Lafitte, a few blocks away. They performed together on seven occasions between 1833 and 1841. The first, on 2 April 1833, was at a benefit concert organized by Hector Berlioz for his bankrupt Shakespearean actress wife Harriet Smithson, during which they played George Onslow's Sonata in F minor for piano duet. Later joint appearances included a benefit concert for the Benevolent Association of Polish Ladies in Paris. Their last appearance together in public was for a charity concert conducted for the Beethoven Memorial in Bonn, held at the Salle Pleyel and the Paris Conservatory on 25 and 26 April 1841.\", 'Beyoncé has received numerous awards. As a solo artist she has sold over 15 million albums in the US, and over 118 million records worldwide (a further 60 million additionally with Destiny\\'s Child), making her one of the best-selling music artists of all time. The Recording Industry Association of America (RIAA) listed Beyoncé as the top certified artist of the 2000s, with a total of 64 certifications. Her songs \"Crazy in Love\", \"Single Ladies (Put a Ring on It)\", \"Halo\", and \"Irreplaceable\" are some of the best-selling singles of all time worldwide. In 2009, The Observer named her the Artist of the Decade and Billboard named her the Top Female Artist and Top Radio Songs Artist of the Decade. In 2010, Billboard named her in their \"Top 50 R&B/Hip-Hop Artists of the Past 25 Years\" list at number 15. In 2012 VH1 ranked her third on their list of the \"100 Greatest Women in Music\". Beyoncé was the first female artist to be honored with the International Artist Award at the American Music Awards. She has also received the Legend Award at the 2008 World Music Awards and the Billboard Millennium Award at the 2011 Billboard Music Awards.']\n",
      "size of contexts: 5\n",
      "answers: {'text': ['the Protestant establishment'], 'answer_start': [297]}\n",
      "answers: {'text': ['Missy Elliott and Alicia Keys'], 'answer_start': [100]}\n",
      "answers: {'text': ['\"I curse the moment of my departure.\"'], 'answer_start': [500]}\n",
      "answers: {'text': [\"38 Rue de la Chaussée-d'Antin\"], 'answer_start': [88]}\n",
      "answers: {'text': ['64'], 'answer_start': [387]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_squad = squad.map(prepare_train_features, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ed466149ca31f82a\n",
      "Found cached dataset json (/home/guest/r12922050/.cache/huggingface/datasets/json/default-ed466149ca31f82a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 2/2 [00:00<00:00, 468.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# load my dataset\n",
    "from datasets import load_dataset\n",
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "\n",
    "my_datasets = load_dataset('json', data_files=data_files)\n",
    "\n",
    "# trim samples\n",
    "for split in my_datasets.keys():\n",
    "    my_datasets[split] = my_datasets[split].select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '593f14f960d971e294af884f0194b3a7',\n",
       " 'question': '舍本和誰的數據能推算出連星的恆星的質量？',\n",
       " 'paragraphs': [2018, 6952, 8264, 836],\n",
       " 'relevant': 836,\n",
       " 'answer': {'start': 108, 'text': '斯特魯維'}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56cf578daab44d1400b8908a',\n",
       " 'title': 'New_York_City',\n",
       " 'context': \"Major tourist destinations include Times Square; Broadway theater productions; the Empire State Building; the Statue of Liberty; Ellis Island; the United Nations Headquarters; museums such as the Metropolitan Museum of Art; greenspaces such as Central Park and Washington Square Park; Rockefeller Center; the Manhattan Chinatown; luxury shopping along Fifth and Madison Avenues; and events such as the Halloween Parade in Greenwich Village; the Macy's Thanksgiving Day Parade; the lighting of the Rockefeller Center Christmas Tree; the St. Patrick's Day parade; seasonal activities such as ice skating in Central Park in the wintertime; the Tribeca Film Festival; and free performances in Central Park at Summerstage. Major attractions in the boroughs outside Manhattan include Flushing Meadows-Corona Park and the Unisphere in Queens; the Bronx Zoo; Coney Island, Brooklyn; and the New York Botanical Garden in the Bronx. The New York Wheel, a 630-foot ferris wheel, was under construction at the northern shore of Staten Island in 2015, overlooking the Statue of Liberty, New York Harbor, and the Lower Manhattan skyline.\",\n",
       " 'question': 'What company sponsors the Thanksgiving Day parade?',\n",
       " 'answers': {'text': [\"Macy's\"], 'answer_start': [445]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load context.json\n",
    "import pandas as pd\n",
    "context_df = pd.read_json('./data/context.json')\n",
    "paragraph_ref_map = {}\n",
    "\n",
    "# enumerate context_df\n",
    "for i, row in context_df.iterrows():\n",
    "    # add context_df to context\n",
    "    paragraph_ref_map[i] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = my_datasets[\"train\"].column_names\n",
    "\n",
    "question_column_name = \"question\" \n",
    "answer_column_name = \"answer\"\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "args = {'doc_stride': 128}\n",
    "\n",
    "max_seq_length = min(384, tokenizer.model_max_length)\n",
    "\n",
    "def modified_prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    questions = [q.lstrip() for q in examples[question_column_name]]\n",
    "    contexts = []\n",
    "    for context_id in examples[\"relevant\"]:\n",
    "        contexts.append(paragraph_ref_map[context_id])\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions if pad_on_right else contexts,\n",
    "        contexts if pad_on_right else questions,\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        # TODO: need to change back to args.doc_stride\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        # TODO: need to change back to args.pad_to_max_length\n",
    "        padding=\"max_length\" if True else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        # check whether answers has start property\n",
    "        if not isinstance(answers[\"start\"], int):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"start\"]\n",
    "            end_char = start_char + len(answers[\"text\"])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 162.20ba/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess my dataset\n",
    "train_datasets = my_datasets[\"train\"]\n",
    "train_datasets = train_datasets.map(modified_prepare_train_features, batched=True, remove_columns=my_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    questions = [q.lstrip() for q in examples[question_column_name]]\n",
    "    contexts = []\n",
    "    for context_id in examples[\"relevant\"]:\n",
    "        contexts.append(paragraph_ref_map[context_id])\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions if pad_on_right else contexts,\n",
    "        contexts if pad_on_right else questions,\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if True else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "    # corresponding example_id and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00, 163.88ba/s]\n"
     ]
    }
   ],
   "source": [
    "eval_examples = my_datasets[\"validation\"]\n",
    "\n",
    "eval_dataset = eval_examples.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=not True,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing:\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=args.version_2_with_negative,\n",
    "        n_best_size=args.n_best_size,\n",
    "        max_answer_length=args.max_answer_length,\n",
    "        null_score_diff_threshold=args.null_score_diff_threshold,\n",
    "        output_dir=args.output_dir,\n",
    "        prefix=stage,\n",
    "        paragraph_ref_map=paragraph_ref_map,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'accelerate.accelerator' has no attribute 'main_process_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb Cell 45\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m accelerator\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m data_files \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m./data/train.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m./data/valid.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m accelerator\u001b[39m.\u001b[39;49mmain_process_first():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     eval_dataset \u001b[39m=\u001b[39m eval_examples\u001b[39m.\u001b[39mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         prepare_validation_features,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning tokenizer on validation dataset\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B3/home/guest/r12922050/GitHub/applied-deep-learning/hw1/test.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m raw_datasets \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m, data_files\u001b[39m=\u001b[39mdata_files)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'accelerate.accelerator' has no attribute 'main_process_first'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=2)\n",
    "\n",
    "data_files = {\n",
    "    'train': './data/train.json',\n",
    "    'validation': './data/valid.json',\n",
    "}\n",
    "with accelerator.main_process_first():\n",
    "    eval_dataset = eval_examples.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on validation dataset\",\n",
    "    )\n",
    "raw_datasets = load_dataset('json', data_files=data_files)\n",
    "eval_examples = raw_datasets[\"validation\"]\n",
    "prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
